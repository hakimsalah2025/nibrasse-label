# üìò Guide Technique NIBRASSE - Syst√®me RAG Avanc√©

## Vue d'ensemble du syst√®me

NIBRASSE est un syst√®me RAG (Retrieval-Augmented Generation) avanc√© optimis√© pour les documents acad√©miques en langue arabe, avec support multilingue (arabe, fran√ßais, anglais).

**Version :** 1.1.0  
**Date :** 26 novembre 2025  
**Nouveaut√© :** Extraction automatique des num√©ros de page dans les citations

---

## üèóÔ∏è Architecture du syst√®me

### Stack Technique

```
Frontend
‚îú‚îÄ‚îÄ HTML5 + CSS3 (Vanilla)
‚îú‚îÄ‚îÄ JavaScript (ES6+)
‚îî‚îÄ‚îÄ Interface responsive

Backend
‚îú‚îÄ‚îÄ FastAPI (Python 3.10+)
‚îú‚îÄ‚îÄ Uvicorn (Serveur ASGI)
‚îî‚îÄ‚îÄ Structure modulaire

Intelligence Artificielle
‚îú‚îÄ‚îÄ Google Gemini Pro (g√©n√©ration)
‚îú‚îÄ‚îÄ Gemini Embedding (vectorisation)
‚îî‚îÄ‚îÄ Gemini 1.5 Pro (reranking)

Bases de donn√©es
‚îú‚îÄ‚îÄ ChromaDB (base vectorielle)
‚îú‚îÄ‚îÄ Supabase (PostgreSQL)
‚îî‚îÄ‚îÄ BM25 (recherche lexicale)

Traitement de texte
‚îú‚îÄ‚îÄ LangChain (chunking)
‚îú‚îÄ‚îÄ RecursiveCharacterTextSplitter
‚îî‚îÄ‚îÄ Regex (extraction de m√©tadonn√©es)
```

---

## üîç Pipeline RAG D√©taill√©

### 1. Ingestion des documents

```python
# Fichier: backend/app/services/ingestion.py

def process_document(file_path: str):
    """
    Processus complet d'ingestion:
    1. Lecture du fichier
    2. Extraction des num√©ros de page ‚ú® NOUVEAU
    3. D√©coupage intelligent (chunking)
    4. G√©n√©ration des embeddings
    5. Stockage multi-base de donn√©es
    """
```

#### 1.1 Extraction des num√©ros de page (‚ú® Nouvelle fonctionnalit√©)

```python
def extract_page_number(text: str) -> str:
    """
    Extrait le num√©ro de page √† partir de diff√©rents formats:
    - --- ÿµŸÅÿ≠ÿ© 123 (OCR) ---
    - --- ÿµŸÅÿ≠ÿ© 123 ---
    - ÿµŸÅÿ≠ÿ© 123
    - ÿµ 123
    
    Retourne:
        str: Num√©ro de page ou None
    """
    patterns = [
        r'---\s*ÿµŸÅÿ≠ÿ©\s+(\d+)\s*\(OCR\)\s*---',
        r'---\s*ÿµŸÅÿ≠ÿ©\s+(\d+)\s*---',
        r'ÿµŸÅÿ≠ÿ©\s+(\d+)',
        r'ÿµ\s*\.?\s*(\d+)',
    ]
    
    for pattern in patterns:
        match = re.search(pattern, text)
        if match:
            return match.group(1)
    return None
```

**Logique d'attribution des pages:**
- Chaque chunk v√©rifie s'il contient une marque de page
- Si oui ‚Üí enregistre ce num√©ro
- Si non ‚Üí h√©rite du dernier num√©ro de page connu
- R√©sultat : continuit√© des num√©ros de page m√™me en cas de chunks longs

#### 1.2 Chunking intelligent

```python
def chunk_text(text: str) -> list[dict]:
    """
    D√©coupage optimis√© pour l'arabe:
    - Taille: 512 caract√®res (√©quilibre contexte/pr√©cision)
    - Chevauchement: 150 caract√®res (√©vite perte de contexte)
    - S√©parateurs: priorit√© paragraphes > phrases > mots
    
    Retourne:
        Liste de dictionnaires avec:
        {
            "text": "contenu...",
            "index": 0,
            "page_number": "256",  # ‚ú® NOUVEAU
            "has_page_marker": True  # ‚ú® NOUVEAU
        }
    """
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=512,
        chunk_overlap=150,
        separators=["\n\n", "\n", "„ÄÇ", ".", " ", ""]
    )
    
    text_chunks = text_splitter.split_text(text)
    
    current_page = None
    chunks_with_metadata = []
    
    for i, chunk in enumerate(text_chunks):
        page_num = extract_page_number(chunk)
        if page_num:
            current_page = page_num
        
        chunks_with_metadata.append({
            "text": chunk,
            "index": i,
            "page_number": current_page,
            "has_page_marker": bool(page_num)
        })
    
    return chunks_with_metadata
```

#### 1.3 Stockage multi-base

```python
# ChromaDB: Stockage vectoriel
metadatas = [{
    "document_id": doc_id,
    "chunk_index": i,
    "filename": filename,
    "page_number": chunk_dict.get("page_number"),  # ‚ú®
    "has_page_marker": chunk_dict.get("has_page_marker")  # ‚ú®
}]

add_documents_to_chroma(
    ids=chroma_ids,
    documents=chunks,
    metadatas=metadatas,
    embeddings=embeddings
)

# Supabase: M√©tadonn√©es structur√©es
supabase.table("chunk").insert({
    "document_id": doc_id,
    "chunk_index": i,
    "content": chunk_dict["text"],
    "embedding_id": chroma_ids[i],
    "metadata": {  # ‚ú® Stock√© en JSON
        "page_number": chunk_dict.get("page_number"),
        "has_page_marker": chunk_dict.get("has_page_marker")
    }
})

# BM25: Index lexical
bm25_service.build_index(
    corpus=current_corpus,
    metadatas=current_metadatas
)
```

---

### 2. Recherche Hybride

```python
# Fichier: backend/app/services/rag.py

def hybrid_search(query: str, top_k: int = 10):
    """
    Recherche hybride combinant:
    1. Recherche s√©mantique (ChromaDB)
    2. Recherche lexicale (BM25)
    3. Fusion RRF (Reciprocal Rank Fusion)
    """
```

#### 2.1 Recherche s√©mantique

```python
# Vectorisation de la requ√™te
query_embedding = get_embedding(query, is_query=True)

# Recherche dans ChromaDB
results = collection.query(
    query_embeddings=[query_embedding],
    n_results=top_k,
    include=['documents', 'metadatas', 'distances']
)
```

#### 2.2 Recherche lexicale BM25

```python
# Tokenisation
tokenized_query = query.split(" ")

# Scoring BM25
bm25_scores = bm25_service.bm25.get_scores(tokenized_query)

# Top-k r√©sultats
top_indices = np.argsort(bm25_scores)[::-1][:top_k]
```

#### 2.3 Fusion RRF

```python
def reciprocal_rank_fusion(results_list, k=60):
    """
    Formule RRF: score = Œ£(1 / (k + rank))
    
    Avantages:
    - Sans param√®tres √† ajuster
    - Robuste aux diff√©rences d'√©chelle
    - Combine efficacement sources h√©t√©rog√®nes
    """
    fused_scores = {}
    for results in results_list:
        for rank, doc_id in enumerate(results):
            if doc_id not in fused_scores:
                fused_scores[doc_id] = 0
            fused_scores[doc_id] += 1 / (k + rank + 1)
    
    return sorted(fused_scores.items(), 
                  key=lambda x: x[1], 
                  reverse=True)
```

---

### 3. Reranking avec Gemini

```python
def rerank_with_gemini(query: str, chunks: list, top_k: int = 3):
    """
    Utilise Gemini 1.5 Pro pour √©valuer la pertinence:
    - Analyse s√©mantique profonde
    - Compr√©hension contextuelle
    - Scoring sur √©chelle 0-10
    """
    prompt = f"""√âvalue la pertinence de chaque passage...
    
    Question: {query}
    
    Passages:
    {numbered_chunks}
    
    Retourne JSON: [{{"passage": 1, "score": 8.5}}, ...]
    """
    
    response = model.generate_content(prompt)
    scores = parse_scores(response.text)
    
    return sorted(scores, key=lambda x: x['score'], reverse=True)[:top_k]
```

---

### 4. G√©n√©ration de r√©ponse (‚ú® Am√©lior√©)

```python
def generate_answer(query: str, context: str, metadatas: list) -> str:
    """
    G√©n√®re une r√©ponse structur√©e avec citations et num√©ros de page
    """
```

#### 4.1 Pr√©paration du contexte enrichi

```python
# ‚ú® Inclusion des num√©ros de page dans les titres
for i, chunk in enumerate(context_chunks, 1):
    filename = metadatas[i-1].get('filename', f'source {i}')
    title = filename.replace('.txt', '')
    
    # ‚ú® NOUVEAU: Ajout automatique du num√©ro de page
    page_number = metadatas[i-1].get('page_number')
    if page_number:
        title_with_page = f"{title} (page {page_number})"
    else:
        title_with_page = title
    
    numbered_context += f"\n### [Source {i}: {title_with_page}]\n{chunk}\n"
```

#### 4.2 Prompt engineering optimis√©

```python
prompt = f"""Tu es un chercheur acad√©mique sp√©cialis√©...

**Instructions CRITIQUES pour les r√©f√©rences:**
1. Utilise le format: [N] Titre (page X) quand disponible
2. Le num√©ro de page est indiqu√© dans le titre de chaque source ci-dessus
3. Si pas de num√©ro de page: [N] Titre seulement
4. Cite pr√©cis√©ment, ne pas inventer

**Structure obligatoire:**
1. Introduction (2-3 lignes, sans titre)
2. Paragraphes avec citations:
   - Explication compl√®te
   - Phrase d'introduction √† la citation
   - Citation textuelle entre guillemets
   - R√©f√©rence [N] sur ligne s√©par√©e
3. Liste des r√©f√©rences:
   **R√©f√©rences:**
   [1] Titre (page X) si disponible
   [2] Titre (page Y) si disponible

**Sources disponibles:**
{numbered_context}

**Question:** {query}
"""
```

#### 4.3 Post-traitement

```python
# S√©paration citations/r√©f√©rences
answer = re.sub(
    r'(["\u201d\u201c¬ª])\s*(\[\d+\])', 
    r'\1\n\2', 
    answer
)

# Formatage des listes
answer = answer.replace('- ', '\n- ')

return answer
```

---

## üìä Performance et Optimisations

### M√©triques cl√©s

```
Temps de traitement (document moyen 50 pages):
‚îú‚îÄ‚îÄ Upload + OCR: ~2-3 secondes
‚îú‚îÄ‚îÄ Chunking + extraction pages: ~1-2 secondes
‚îú‚îÄ‚îÄ Embeddings (batch): ~3-5 secondes
‚îú‚îÄ‚îÄ Stockage DB: ~1-2 secondes
‚îî‚îÄ‚îÄ Total ingestion: ~7-12 secondes

Temps de requ√™te:
‚îú‚îÄ‚îÄ Recherche hybride: ~0.5-1 seconde
‚îú‚îÄ‚îÄ Reranking Gemini: ~2-3 secondes
‚îú‚îÄ‚îÄ G√©n√©ration r√©ponse: ~5-10 secondes
‚îî‚îÄ‚îÄ Total query: ~8-15 secondes
```

### Optimisations impl√©ment√©es

1. **Batch embeddings:** Vectorisation group√©e pour r√©duire les appels API
2. **Caching:** Mise en cache des embeddings fr√©quents
3. **Index BM25:** Pr√©-calcul pour recherche instantan√©e
4. **Chunk size optimis√©:** 512 caract√®res = √©quilibre contexte/pr√©cision
5. **Lazy loading:** Initialisation des mod√®les √† la demande

---

## üîß Configuration et D√©ploiement

### Variables d'environnement

```env
# backend/.env

# API Keys
GEMINI_API_KEY=votre_cle_gemini
VITE_SUPABASE_URL=https://votre-projet.supabase.co
VITE_SUPABASE_ANON_KEY=votre_cle_anon

# Mod√®les Gemini
GEMINI_CHAT_MODEL=gemini-1.5-pro-002
GEMINI_EMBEDDING_MODEL=models/text-embedding-004

# Param√®tres RAG
CHUNK_SIZE=512
CHUNK_OVERLAP=150
TOP_K_RESULTS=10
RERANK_TOP_K=3
```

### Structure des bases de donn√©es

#### Supabase Schema

```sql
-- Table documents
CREATE TABLE documents (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    filename TEXT NOT NULL,
    upload_date TIMESTAMP DEFAULT NOW(),
    total_chunks INTEGER DEFAULT 0,
    file_size INTEGER,
    file_type TEXT,
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW()
);

-- Table chunks
CREATE TABLE chunk (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    document_id UUID REFERENCES documents(id) ON DELETE CASCADE,
    chunk_index INTEGER NOT NULL,
    content TEXT NOT NULL,
    embedding_id TEXT,
    created_at TIMESTAMP DEFAULT NOW(),
    metadata JSONB  -- ‚ú® Contient page_number et has_page_marker
);

-- Indexes pour performance
CREATE INDEX idx_chunks_document_id ON chunk(document_id);
CREATE INDEX idx_chunks_chunk_index ON chunk(chunk_index);
CREATE INDEX idx_documents_filename ON documents(filename);
```

#### ChromaDB Collections

```python
collection = client.get_or_create_collection(
    name="rag_collection",
    embedding_function=None,  # Embeddings pr√©-calcul√©s
    metadata={"hnsw:space": "cosine"}
)

# M√©tadonn√©es stock√©es:
{
    "document_id": "uuid",
    "chunk_index": 0,
    "filename": "document.txt",
    "page_number": "256",  # ‚ú® NOUVEAU
    "has_page_marker": True  # ‚ú® NOUVEAU
}
```

---

## üß™ Tests et Validation

### Tests unitaires

```python
# backend/tests/test_page_extraction.py

def test_extract_page_number_ocr():
    """Teste extraction format OCR complet"""
    text = "--- ÿµŸÅÿ≠ÿ© 256 (OCR) ---\nContenu..."
    assert extract_page_number(text) == "256"

def test_chunk_text_with_pages():
    """Teste chunking avec pr√©servation num√©ros de page"""
    chunks = chunk_text(sample_content)
    assert all('page_number' in c for c in chunks)
    assert chunks[0]['page_number'] is not None
```

Lancer les tests:
```bash
cd backend
python tests/test_page_extraction.py
```

### Tests d'int√©gration

```bash
# Test du pipeline complet
python tests/test_full_rag_pipeline.py

# Test avec document r√©el
python tests/test_with_real_document.py
```

---

## üîç D√©bogage et Monitoring

### Logs d√©taill√©s

```python
# Dans ingestion.py
print(f"‚úÖ Document process√©: {filename}")
print(f"   - Chunks totaux: {len(chunks)}")
print(f"   - Chunks avec pages: {chunks_with_pages}")
print(f"   - Taux de couverture: {chunks_with_pages/len(chunks)*100:.1f}%")

# Dans rag.py
print(f"üîç Recherche pour: {query}")
print(f"   - R√©sultats hybrides: {len(hybrid_results)}")
print(f"   - Apr√®s reranking: {len(reranked)}")
print(f"   - Num√©ros de page trouv√©s: {sum(1 for m in metadatas if m.get('page_number'))}")
```

### Endpoints de diagnostic

```python
# GET /api/stats - Statistiques syst√®me
{
    "total_documents": 42,
    "total_chunks": 1234,
    "chunks_with_pages": 987,
    "coverage_rate": "80.0%"
}

# GET /api/health - √âtat du syst√®me
{
    "status": "healthy",
    "chromadb": "connected",
    "supabase": "connected",
    "bm25": "indexed"
}
```

---

## üìö R√©f√©rences techniques

### Documentation externe

- **LangChain:** https://python.langchain.com/
- **ChromaDB:** https://docs.trychroma.com/
- **Supabase:** https://supabase.com/docs
- **Google Gemini:** https://ai.google.dev/docs

### Papiers de recherche

1. **RAG:** "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks" (Lewis et al., 2020)
2. **BM25:** "The Probabilistic Relevance Framework: BM25 and Beyond" (Robertson & Zaragoza, 2009)
3. **RRF:** "Reciprocal Rank Fusion outperforms Condorcet and individual Rank Learning Methods" (Cormack et al., 2009)

---

## üîÑ Roadmap Technique

### Version 1.2.0 (Planifi√©e)
- [ ] Support PDF natif (sans conversion)
- [ ] Extraction automatique de tables
- [ ] Am√©lioration chunking s√©mantique
- [ ] Cache Redis pour embeddings

### Version 1.3.0 (Future)
- [ ] Multi-modal (images + texte)
- [ ] Graphes de connaissances
- [ ] Fine-tuning mod√®le embedding
- [ ] API REST compl√®te

---

**Maintenu par:** √âquipe NIBRASSE  
**Derni√®re mise √† jour:** 26 novembre 2025  
**Version:** 1.1.0
